{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wellecks/llmstep/blob/colab/python/colab/llmstep_colab_server.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXc849NMr4Ks"
      },
      "source": [
        "# [`llmstep`](https://github.com/wellecks/llmstep) server using Colab notebook\n",
        "In order to use this notebook, follow these instructions:\n",
        "\n",
        "1. Run all the cells in this colab notebook to start your server.\n",
        "\n",
        "2. In your local environment, set the environment variable `LLMSTEP_HOST` equal to the url printed out in this notebook (for example, https://04fa-34-125-110-83.ngrok.io/)\n",
        "\n",
        "3. In your local environment, set the environment variable `LLMSTEP_SERVER=COLAB`.\n",
        "\n",
        "4. Use `llmstep`.\n",
        "\n",
        "\n",
        "#### VSCode steps (2) and (3)\n",
        "\n",
        "To set environment variables in VS Code, go to:\n",
        "\n",
        "- Settings (`Command` + `,` on Mac)\n",
        "- Extensions -> Lean 4\n",
        "- Add environment variables to `Server Env`.\n",
        "- Then restart the Lean Server (`Command` + `t`, then type `> Lean 4: Restart Server`)\n",
        "\n",
        "\n",
        "Authors: Rahul Saha, Sean Welleck"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-WRNlphi1A_"
      },
      "source": [
        "## Configuration\n",
        "\n",
        "First we configure the model and generation parameters.\n",
        "\n",
        "Below we set default values; modify them if needed for your setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M52N1j8i0Cg"
      },
      "outputs": [],
      "source": [
        "# Prompt template for the default model.\n",
        "# Change this if your model expects a different input format.\n",
        "def llmstep_prompt(tactic_state, prefix):\n",
        "  return '[GOAL]%s[PROOFSTEP]%s' % (tactic_state, prefix)\n",
        "\n",
        "\n",
        "CONFIG = {\n",
        "    'LLMSTEP_MODEL': 'wellecks/llmstep-mathlib4-pythia2.8b',\n",
        "\n",
        "    # Sampling temperature(s)\n",
        "    'LLMSTEP_TEMPERATURES': [0.5],\n",
        "\n",
        "    # Number of generated suggestions per temperature\n",
        "    'LLMSTEP_NUM_SAMPLES': 10,\n",
        "\n",
        "    # Prompt template\n",
        "    'LLMSTEP_PROMPT': llmstep_prompt\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQT-yfcPkTAv"
      },
      "source": [
        "### Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64ET5m-IxV3U"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok\n",
        "!pip install flask\n",
        "!pip install transformers\n",
        "!pip install flask_ngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJdJ-2N8lt_0"
      },
      "source": [
        "### Implementation of server and model utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "628g7MXat9gv"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import transformers\n",
        "import torch\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from http.server import BaseHTTPRequestHandler, HTTPServer\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "def load_hf(hf_model):\n",
        "    print(\"Loading model...\")\n",
        "    if 'wellecks/llmstep-mathlib4-pythia' in hf_model:\n",
        "        model = transformers.GPTNeoXForCausalLM.from_pretrained(\n",
        "            hf_model,\n",
        "            torch_dtype=torch.float16\n",
        "        )\n",
        "        tokenizer = transformers.GPTNeoXTokenizerFast.from_pretrained(hf_model)\n",
        "    else:\n",
        "        raise NotImplementedError(hf_model)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    model.eval()\n",
        "    print(\"Done.\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def hf_generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    temperatures,\n",
        "    num_samples,\n",
        "    max_new_tokens=128\n",
        "):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "    texts = []\n",
        "    for temp in temperatures:\n",
        "        out = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=temp > 0,\n",
        "            temperature=temp,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            num_return_sequences=num_samples if temp > 0 else 1\n",
        "        )\n",
        "        output_tokens = out[:, input_ids.shape[1]:]\n",
        "        texts.extend(tokenizer.batch_decode(\n",
        "            output_tokens,\n",
        "            skip_special_tokens=True\n",
        "        ))\n",
        "    texts = list(set(texts))\n",
        "    return texts\n",
        "\n",
        "\n",
        "class LLMStepServer(HTTPServer):\n",
        "    def __init__(\n",
        "        self, model, tokenizer, generate_function, config, *args, **kwargs\n",
        "    ):\n",
        "      self.model = model\n",
        "      self.tokenizer = tokenizer\n",
        "      self.generate_function = generate_function\n",
        "      self.config = config\n",
        "      super().__init__(*args, **kwargs)\n",
        "\n",
        "\n",
        "class LLMStepRequestHandler(BaseHTTPRequestHandler):\n",
        "    def process_request(self, tactic_state, prefix):\n",
        "        prompt = self.server.config['LLMSTEP_PROMPT'](tactic_state, prefix)\n",
        "        texts = self.server.generate_function(\n",
        "            model=self.server.model,\n",
        "            tokenizer=self.server.tokenizer,\n",
        "            prompt=prompt,\n",
        "            temperatures=self.server.config['LLMSTEP_TEMPERATURES'],\n",
        "            num_samples=self.server.config['LLMSTEP_NUM_SAMPLES']\n",
        "        )\n",
        "        texts = [prefix + text for text in texts]\n",
        "        response = {\"suggestions\": texts}\n",
        "        return response\n",
        "\n",
        "    def do_POST(self):\n",
        "        # Set response headers\n",
        "        self.send_response(200)\n",
        "        self.send_header('Content-type', 'application/json')\n",
        "        self.end_headers()\n",
        "\n",
        "        # Get the incoming POST data\n",
        "        content_length = int(self.headers['Content-Length'])\n",
        "        post_data = self.rfile.read(content_length).decode('utf-8')\n",
        "\n",
        "        try:\n",
        "            data = json.loads(post_data)\n",
        "            result = self.process_request(data['tactic_state'], data['prefix'])\n",
        "            response = result\n",
        "            self.wfile.write(json.dumps(response).encode('utf-8'))\n",
        "        except Exception as e:\n",
        "            # Handle errors gracefully\n",
        "            error_response = {'error': str(e)}\n",
        "            self.wfile.write(json.dumps(error_response).encode('utf-8'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xjQsG895sdB"
      },
      "source": [
        "### Run : load the model and start the server\n",
        "\n",
        "The cell prints out the public URL, for instance: https://04fa-34-125-110-73.ngrok.io\n",
        "\n",
        "Add this URL as a `LLMSTEP_HOST` environment variable in your local environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XnQqAH3at-VR"
      },
      "outputs": [],
      "source": [
        "# Download and load the model (this takes a few minutes).\n",
        "model, tokenizer = load_hf(CONFIG['LLMSTEP_MODEL'])\n",
        "model.cuda();\n",
        "\n",
        "PORT = 81\n",
        "\n",
        "# Open a HTTP tunnel\n",
        "public_url = ngrok.connect(PORT)\n",
        "print('Your public url is:\\n%s\\n\\nSet LLMSTEP_HOST to this url.' % public_url)\n",
        "\n",
        "# Create the server\n",
        "server_address = ('', PORT)\n",
        "httpd = LLMStepServer(\n",
        "    model, tokenizer, hf_generate, CONFIG,\n",
        "    server_address, LLMStepRequestHandler\n",
        ")\n",
        "\n",
        "print('Server started')\n",
        "httpd.serve_forever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hI381f5wq6_G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
